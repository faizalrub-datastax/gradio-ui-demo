{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demo UI"
      ],
      "metadata": {
        "id": "kA3U25-l-ho_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table Schema\n",
        "\n",
        "```\n",
        "CREATE TABLE IF NOT EXISTS workspan.customer_opportunities (\n",
        "    customer_id text,\n",
        "    partner_id text,\n",
        "    opportunity_id text,\n",
        "    customer_name text static,\n",
        "    next_step text,\n",
        "    cadence text,\n",
        "    llm_output text,\n",
        "    opportunity map<text, text>,\n",
        "    llm_output_embedding vector<float, 1536>,\n",
        "    sentiment text,\n",
        "    PRIMARY KEY ((customer_id, partner_id), opportunity_id)\n",
        ") WITH CLUSTERING ORDER BY (opportunity_id DESC)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "wwm-PpEAFunq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* customer_id: Unique identifier for each customer\n",
        "* partner_id: AWS / Azure / GCP\n",
        "* opportunity_id: Unique identifier for each opportunity\n",
        "* opportunity: Dynamic collection of data field names and corresponding values specific to the opportunity. Other fields can be stored in separate table columns.\n",
        "* llm_ouput: LLM output summarizing the 'next steps' , 'challenges' and 'open items'  \n",
        "* llm_ouput_embedding: Text embeddings corresponding to llm_output\n",
        "* sentiment: Positive, Neutral and Negative sentiment derived from 'next step' and 'cadence' data fields. This can be determined either by using a sentiment analysis library such as  Python Natural Language Toolkit (NLTK) or from LLM.\n"
      ],
      "metadata": {
        "id": "dvkUd8WMF1n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "JrOCKrYWF8aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai cassandra-driver llama-index gradio"
      ],
      "metadata": {
        "id": "XTKDxqv2-_D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "from cassandra.query import dict_factory\n",
        "from cassandra.query import SimpleStatement\n",
        "import openai\n",
        "from llama_index import ListIndex\n",
        "from llama_index.readers.schema.base import Document\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "WgpHx7Kl-_1z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keys & Environment Variables"
      ],
      "metadata": {
        "id": "gfGDACn1GAiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keys and tokens here\n",
        "openai_api_key = \"\"\n",
        "openai.api_key = openai_api_key\n",
        "cass_user = ''\n",
        "cass_pw = ''\n",
        "scb_path = 'secure-connect-vector-search-demo.zip'"
      ],
      "metadata": {
        "id": "grC2q4rL_F5z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select a model to compute embeddings"
      ],
      "metadata": {
        "id": "u6Hhr9fvGFrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"text-embedding-ada-002\""
      ],
      "metadata": {
        "id": "BxMgin-q_MfK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to the Cluster"
      ],
      "metadata": {
        "id": "9RfFV600GLzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cloud_config= {\n",
        "  'secure_connect_bundle': scb_path\n",
        "}\n",
        "auth_provider = PlainTextAuthProvider(cass_user, cass_pw)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "session.set_keyspace('workspan')"
      ],
      "metadata": {
        "id": "s9PDGGx8_OW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_queries_across_opportunities():\n",
        "    # This function returns a list of queries.\n",
        "    # You can modify this to fetch the options from a different source if required.\n",
        "    queries = ['Identify the wins',\n",
        "               'Identify opportunities with next step to schedule a meeting',\n",
        "               'I want to know more about the customer and the challenges']\n",
        "    return queries"
      ],
      "metadata": {
        "id": "esrKpvNAmXuw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_queries_specific_to_opportunity():\n",
        "    # This function returns a list of queries.\n",
        "    # You can modify this to fetch the options from a different source if required.\n",
        "    queries = ['What is the sentiment on this opportunity?',\n",
        "               'What are the next steps for this opportunity?',\n",
        "               'What are the open items for this opportunity?',\n",
        "               'What are the challenges for this opportunity?']\n",
        "    return queries"
      ],
      "metadata": {
        "id": "ihk-BX6P95hS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Opportunity Specific Queries"
      ],
      "metadata": {
        "id": "jyAwl_5vGhB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_opportunity_sentiment(opportunity):\n",
        "    # This function returns a list of opportunities.\n",
        "    # You can modify this to fetch the options from a different source if required.\n",
        "    cqlSelect = f'''SELECT * FROM workspan.customer_opportunities WHERE customer_id = 'CUS100' and partner_id = 'AWS' and opportunity_id = '{opportunity}' ;'''\n",
        "    rows = session.execute(cqlSelect)\n",
        "    results = []\n",
        "    for row_i, row in enumerate(rows):\n",
        "        results.append(row.sentiment)\n",
        "    return results[0]"
      ],
      "metadata": {
        "id": "-_NmQzv_dZLH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def query_opportunity_nextsteps(opportunity):\n",
        "    # This function returns a list of opportunities.\n",
        "    # You can modify this to fetch the options from a different source if required.\n",
        "    cqlSelect = f'''SELECT * FROM workspan.customer_opportunities WHERE customer_id = 'CUS100' and partner_id = 'AWS' and opportunity_id = '{opportunity}' ;'''\n",
        "    rows = session.execute(cqlSelect)\n",
        "    results = []\n",
        "    for row_i, row in enumerate(rows):\n",
        "        results.append(row.llm_output)\n",
        "\n",
        "    next_steps = [\"Next Steps section not found!\"]\n",
        "    # Find the section for \"Next Steps\"\n",
        "    match = re.search(r'Next Steps:(.*?)(?=Challenges:|Open Items:|$)', results[0], re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    if match:\n",
        "        next_steps_section = match.group(1).strip()\n",
        "        next_steps = [step.strip('- ').strip() for step in next_steps_section.split('\\n') if step]\n",
        "\n",
        "    next_steps_str = \" \".join([step if step.endswith('.') else step + '.' for step in next_steps])\n",
        "\n",
        "    return next_steps_str\n"
      ],
      "metadata": {
        "id": "7xIa8FCudyWx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_opportunity_openitems(opportunity):\n",
        "    # This function returns a list of opportunities.\n",
        "    # You can modify this to fetch the options from a different source if required.\n",
        "    cqlSelect = f'''SELECT * FROM workspan.customer_opportunities WHERE customer_id = 'CUS100' and partner_id = 'AWS' and opportunity_id = '{opportunity}' ;'''\n",
        "    rows = session.execute(cqlSelect)\n",
        "    results = []\n",
        "    for row_i, row in enumerate(rows):\n",
        "        results.append(row.llm_output)\n",
        "\n",
        "    open_items = [\"Open Items section not found!\"]\n",
        "    # Find the section for \"Open Items\"\n",
        "    match = re.search(r'Open Items:(.*?)(?=Challenges:|Next Steps:|$)', results[0], re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    if match:\n",
        "        open_items_section = match.group(1).strip()\n",
        "        open_items = [item.strip('- ').strip() for item in open_items_section.split('\\n') if item]\n",
        "\n",
        "    open_items_str = \" \".join([item if item.endswith('.') else item + '.' for item in open_items])\n",
        "\n",
        "    return open_items_str"
      ],
      "metadata": {
        "id": "kwwRbxyOd-SF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_opportunity_challenges(opportunity):\n",
        "    # This function returns a list of opportunities.\n",
        "    # You can modify this to fetch the options from a different source if required.\n",
        "    cqlSelect = f'''SELECT * FROM workspan.customer_opportunities WHERE customer_id = 'CUS100' and partner_id = 'AWS' and opportunity_id = '{opportunity}' ;'''\n",
        "    rows = session.execute(cqlSelect)\n",
        "    results = []\n",
        "    for row_i, row in enumerate(rows):\n",
        "        results.append(row.llm_output)\n",
        "\n",
        "    challenges = [\"Challenges section not found!\"]\n",
        "    # Find the section for \"Challenges\"\n",
        "    match = re.search(r'Challenges:(.*?)(?=Next Steps:|Open Items:|$)', results[0], re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    if match:\n",
        "        challenges_section = match.group(1).strip()\n",
        "        challenges = [challenge.strip('- ').strip() for challenge in challenges_section.split('\\n') if challenge]\n",
        "\n",
        "    challenges_str = \" \".join([challenge if challenge.endswith('.') else challenge + '.' for challenge in challenges])\n",
        "\n",
        "    return challenges_str"
      ],
      "metadata": {
        "id": "qzCdNwJIeSGk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_opportunities():\n",
        "    # This function returns a list of oppertunities.\n",
        "    # You can modify this to fetch the options from a different source if required.\n",
        "    cqlSelect = f'''SELECT opportunity_id FROM workspan.customer_opportunities WHERE customer_id = 'CUS100' and partner_id = 'AWS'  ;'''\n",
        "    rows = session.execute(cqlSelect)\n",
        "    opportunities = []\n",
        "    for row_i, row in enumerate(rows):\n",
        "        opportunities.append(row.opportunity_id)\n",
        "    return opportunities"
      ],
      "metadata": {
        "id": "ippETBlq95Oy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customer Specific Queries (across multiple opportunities)"
      ],
      "metadata": {
        "id": "KLefbP5kGpmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_customer_wins():\n",
        "    # This function returns a list of opportunities.\n",
        "    # You can modify this to fetch the options from a different source if required.\n",
        "    cqlSelect = f'''SELECT * FROM workspan.customer_opportunities WHERE customer_id = 'CUS100' and partner_id = 'AWS' and sentiment = 'positive'  ;'''\n",
        "    rows = session.execute(cqlSelect)\n",
        "    results = []\n",
        "    for row_i, row in enumerate(rows):\n",
        "        results.append(row.opportunity_id)\n",
        "\n",
        "    results_str = \" \".join([result if result.endswith('.') else result + '.' for result in results])\n",
        "\n",
        "    return results_str"
      ],
      "metadata": {
        "id": "7USpFTutF8g6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_customer_next_step_to_schedule_meeting():\n",
        "    # This function returns a list of opportunities.\n",
        "    # You can modify this to fetch the options from a different source if required.\n",
        "    vectorsearchon = 'next action to set up a meeting'\n",
        "    embedding = openai.Embedding.create(input= vectorsearchon, model=model_id)['data'][0]['embedding']\n",
        "    cqlSelect = f'''SELECT * FROM workspan.customer_opportunities WHERE customer_id = 'CUS100' and partner_id = 'AWS' and llm_output : 'schedule a meeting' ORDER BY llm_output_embedding ANN OF {embedding} LIMIT 10;'''\n",
        "    rows = session.execute(cqlSelect)\n",
        "    results = []\n",
        "    for row_i, row in enumerate(rows):\n",
        "        next_steps = [\"Schedule a Meeting section not found!\"]\n",
        "        # Find the section for \"Next Steps\"\n",
        "        match = re.search(r'Schedule a Meeting:(.*?)(?=Challenges:|Open Items:|$)', row.llm_output, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        if match:\n",
        "            next_steps_section = match.group(1).strip()\n",
        "            next_steps = [step.strip('- ').strip() for step in next_steps_section.split('\\n') if step]\n",
        "\n",
        "        next_steps_str = \" \".join([step if step.endswith('.') else step + '.' for step in next_steps])\n",
        "        results.append(row.opportunity_id + ' : ' + next_steps_str + '\\n' + '\\n')\n",
        "\n",
        "    results_str = \" \".join([result if result.endswith('.') else result + '.' for result in results])\n",
        "\n",
        "    return results_str"
      ],
      "metadata": {
        "id": "LjAf4NVZIdlx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_customer_challenges():\n",
        "    vectorsearchon = 'find opportunity with listed challenges'\n",
        "    embedding = openai.Embedding.create(input= vectorsearchon, model=model_id)['data'][0]['embedding']\n",
        "\n",
        "    cqlSelect = f'''SELECT llm_output FROM workspan.customer_opportunities WHERE customer_id = 'CUS100' and partner_id = 'AWS' ORDER BY llm_output_embedding ANN OF {embedding} LIMIT 10;  '''\n",
        "    rows = session.execute(cqlSelect)\n",
        "    #print(rows)\n",
        "    documents = []\n",
        "    for item in rows:\n",
        "        documents.append(Document(text=str(item)))\n",
        "        #print(str(item))\n",
        "\n",
        "    index = ListIndex.from_documents(documents)\n",
        "\n",
        "    # set Logging to DEBUG for more detailed outputs\n",
        "    query_engine = index.as_query_engine()\n",
        "    response = query_engine.query(\"What are the Challenges?\")\n",
        "\n",
        "    return f\"{response}\""
      ],
      "metadata": {
        "id": "XWWmmhWBKyfU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_customer_llmoutput():\n",
        "    cqlSelect = f'''SELECT llm_output FROM workspan.customer_opportunities WHERE customer_id = 'CUS100' and partner_id = 'AWS' ;  '''\n",
        "    rows = session.execute(cqlSelect)\n",
        "    llmoutputs = []\n",
        "\n",
        "    for item in rows:\n",
        "        llmoutputs.append(Document(text=str(item)))\n",
        "\n",
        "    return llmoutputs"
      ],
      "metadata": {
        "id": "X7Bo5TyaER-6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to be called on button click\n",
        "def on_oppo_button_click(opportunity,query):\n",
        "    if query == \"What is the sentiment on this opportunity?\":\n",
        "        result = query_opportunity_sentiment(opportunity)\n",
        "    elif query == \"What are the next steps for this opportunity?\":\n",
        "        result = query_opportunity_nextsteps(opportunity)\n",
        "    elif query == \"What are the open items for this opportunity?\":\n",
        "        result = query_opportunity_openitems(opportunity)\n",
        "    elif query == \"What are the challenges for this opportunity?\":\n",
        "        result = query_opportunity_challenges(opportunity)\n",
        "    return result"
      ],
      "metadata": {
        "id": "81gGP981945e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to be called on button click\n",
        "def on_cus_button_click(query):\n",
        "    if query == \"Identify the wins\":\n",
        "        result = query_customer_wins()\n",
        "    elif query == \"Identify opportunities with next step to schedule a meeting\":\n",
        "        result = query_customer_next_step_to_schedule_meeting()\n",
        "    elif query == \"I want to know more about the customer and the challenges\":\n",
        "        result = query_customer_challenges()\n",
        "    return result"
      ],
      "metadata": {
        "id": "B1b7yj11FEnQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UI Demo - Pre-determined queries"
      ],
      "metadata": {
        "id": "B2M5G5t9HDLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "# Logo and title using the Markdown component\n",
        "logo_markdown = \"![Company Logo]\"\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Glass(),css=\".gradio-container {background-color: lightgrey}\") as demo:\n",
        "    gr.Markdown(logo_markdown)  # Adding the logo and title\n",
        "    # List of items for the dropdown\n",
        "    dropdown_opportunity = gr.inputs.Dropdown(choices=get_opportunities(), label=\"Select an opportunity:\")\n",
        "    dropdown_query = gr.inputs.Dropdown(choices=get_queries_specific_to_opportunity(), label=\"Select Query:\")\n",
        "    output = gr.Textbox(label=\"Output Box\")\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    submit_btn.click(fn=on_oppo_button_click, inputs=[dropdown_opportunity, dropdown_query], outputs=output, api_name=\"on_oppo_button_click\")\n",
        "\n",
        "    dropdown_cus_query = gr.inputs.Dropdown(choices=get_queries_across_opportunities(), label=\"Select Query:\")\n",
        "    output_cus = gr.Textbox(label=\"Output Box\")\n",
        "    submit_cus_btn = gr.Button(\"Submit\")\n",
        "    submit_cus_btn.click(fn=on_cus_button_click, inputs=dropdown_cus_query, outputs=output_cus, api_name=\"on_cus_button_click\")\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "bxMBwZvi0HTz",
        "outputId": "b768118e-778f-4e1a-8686-693e8c071c23"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-3bd8f76ea77e>:10: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  dropdown_opportunity = gr.inputs.Dropdown(choices=get_opportunities(), label=\"Select an opportunity:\")\n",
            "<ipython-input-44-3bd8f76ea77e>:10: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  dropdown_opportunity = gr.inputs.Dropdown(choices=get_opportunities(), label=\"Select an opportunity:\")\n",
            "<ipython-input-44-3bd8f76ea77e>:11: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  dropdown_query = gr.inputs.Dropdown(choices=get_queries_specific_to_opportunity(), label=\"Select Query:\")\n",
            "<ipython-input-44-3bd8f76ea77e>:11: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  dropdown_query = gr.inputs.Dropdown(choices=get_queries_specific_to_opportunity(), label=\"Select Query:\")\n",
            "<ipython-input-44-3bd8f76ea77e>:16: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  dropdown_cus_query = gr.inputs.Dropdown(choices=get_queries_across_opportunities(), label=\"Select Query:\")\n",
            "<ipython-input-44-3bd8f76ea77e>:16: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  dropdown_cus_query = gr.inputs.Dropdown(choices=get_queries_across_opportunities(), label=\"Select Query:\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://f315719ee0e735e9b1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f315719ee0e735e9b1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UI Demo - ChatBot with LangChain (with custom queries)"
      ],
      "metadata": {
        "id": "QOJGCLe-HOqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import openai\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key  # Replace with your key\n",
        "\n",
        "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')\n",
        "\n",
        "# Load custom data\n",
        "custom_data = query_customer_llmoutput()\n",
        "\n",
        "def predict(message, history):\n",
        "    # Checking if the message is in custom data\n",
        "    if message == \"What are the customer challenges?\" :\n",
        "        index = ListIndex.from_documents(custom_data)\n",
        "        query_engine = index.as_query_engine()\n",
        "        result = query_engine.query(\"What are the Challenges?\")\n",
        "        response = result.response\n",
        "    else:\n",
        "        history_langchain_format = []\n",
        "        for human, ai in history:\n",
        "            history_langchain_format.append(HumanMessage(content=human))\n",
        "            history_langchain_format.append(AIMessage(content=ai))\n",
        "        history_langchain_format.append(HumanMessage(content=message))\n",
        "        gpt_response = llm(history_langchain_format)\n",
        "        response = gpt_response.content\n",
        "    return response\n",
        "\n",
        "gr.ChatInterface(predict).launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "Jjl9-BM5_U7n",
        "outputId": "63a17ffe-d48f-43b9-e0f3-a7e44ae68980"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://8491d55a04a8218e8f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8491d55a04a8218e8f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UI Demo - ChatBot directly integrating to OpenAI"
      ],
      "metadata": {
        "id": "y2ejDIMUHVTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import gradio as gr\n",
        "\n",
        "openai.api_key = openai_api_key  # Replace with your key\n",
        "\n",
        "#llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')\n",
        "\n",
        "def predict(message, history):\n",
        "    history_openai_format = []\n",
        "    for human, assistant in history:\n",
        "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
        "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
        "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages= history_openai_format,\n",
        "        temperature=1.0,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    partial_message = \"\"\n",
        "    for chunk in response:\n",
        "        if len(chunk['choices'][0]['delta']) != 0:\n",
        "            partial_message = partial_message + chunk['choices'][0]['delta']['content']\n",
        "            yield partial_message\n",
        "\n",
        "gr.ChatInterface(predict).queue().launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "HMABu6vJBZUP",
        "outputId": "ed1c93a2-d9f7-4765-bd34-8a1e1f5d4ab0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://eab7254e333d80f57c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eab7254e333d80f57c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}